---
layout: post
title: "Accuracy metrics for model evaluation"
categories: [machine learning]
math: 1

---

{% include toc.html %}

{% assign img-url = '/img/post/ML' %}

Evaluation metrics are used to explain the performance of a model. Basically, we can compare the *actual values* and the *predicted values* to calculate the accuracy of our models. In this post, I try to understand the meaning and usage of some popular errors in the model evaluation.

{:.alert.alert-warning}
This article is not for you to learn, it's for refrence only!

{:.question}
## What's an error of the model? (regression metrics)

The ***error of the model*** is the difference between the data points and the trend line generated by the algorithm. There are many ways to calculate this difference (regression metrics)

- **Mean Absolute Error** (MAE) : $MAE = \frac{1}{n}\sum\_{j=1}^n \vert y\_j - \hat{y}\_j \vert$.
- **Mean Squared Error** (MSE) : $MSE = \frac{1}{n}\sum\_{j=1}^n (y\_j - \hat{y}\_j)^2$
- **Root Mean Squared Error** (RMSR): $RMSR = \sqrt{\frac{1}{n}\sum\_{j=1}^n (y\_j - \hat{y}\_j)^2}$
- **Relative absolute Error** (RAE): $RAE = \dfrac{\sum\_{j=1}^n\vert y\_j-\hat{y}\_j\vert}{\sum\_{j=1}^n\vert y\_j-\bar{y}\vert}$ ($\bar{y}$ is the mean value of $y$)
- **Relative Squared Error** (RSE): $RSE = \dfrac{\sum\_{j=1}^n (y\_j-\hat{y}\_j)^2}{\sum\_{j=1}^n (y\_j-\bar{y}\_j)^2}$
- **R squared**: $R^2 = 1 - RSE$.

What's their meaning?

- **MAE** : It's just the average error, the easiest one. All individual differences have the same role, there is no one being more weighted than the others.
- **MSE** : It focuses on "larger" errors because of the squared term. The higher this value, the worse the model is.
	- *MSE is more popular than MAE* because in the MAE, all gears are equivalent while in MSE, the bigger gears will influence much on the final error.
- **RMSE** : the most popular because it is interpretable *in the same units* as the response vector or $y$ units.
- **RAE** : It's normalized, i.e. it doesn't depend much on the unit of $y$.
- **RSE** : It's used for calculating $R^2$.
- **$R^2$** : It represents how close the data values are, to the fitted regression line. <mark>The higher the R-squared, the better the model fits your data</mark>.

The choice of metric, completely depends on 

- The type of model, 
- Your data type
- Domain of knowledge.

{:.question}
## How about the metrics for classification models?

- **[Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index){:target="_blank"}** (*Jaccard similarity coefficient*) :

  <div class="columns-2" markdown="1">
  $$
  J(y,\hat{y}) = \dfrac{\vert y\cap \hat{y}\vert}{\vert y\vert + \vert \hat{y}\vert - \vert y\cap \hat{y}\vert}
  $$
  
  {:.img-full-normal}
  ![Jaccard index]({{img-url}}/jaccard.png)
  </div>

	- $0 \le J \le 1$.
	- **<mark>Higher is better</mark>** : The common part is bigger, thus the numerator is bigger and the denominator is smaller.

	~~~ python
  sklearn.metrics.jaccard_similarity_score
	~~~

- **[F1-Score](https://en.wikipedia.org/wiki/F1_score){:target="_blank"}** (*F-score, F-measure*) : I have [another post writing about this](/understand-confusion-matrix-and-f1-score){:target="_blank"}.

	$$
	\begin{align}
	F_{1} &= \left({\frac {\mathrm {recall} ^{-1}+\mathrm {precision} ^{-1}}{2}}\right)^{-1}=2\times {\frac {\mathrm {precision} \cdot \mathrm {recall} }{\mathrm {precision} +\mathrm {recall} }}.\\
	\mathrm {precision} &= \dfrac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}, \, \mathrm{TP} = \mathrm{True\,Positive}, \mathrm{FP} = \mathrm{False\,Positive}. \\
	\mathrm {recall} &= \dfrac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}, \, \mathrm{FN} = \mathrm{False\,Negative}.
	\end{align}
	$$

	{:.img-full-75}
	![Precision and recall in F1-Score]({{img-url}}/precision_recall.png)

	- $0 \le F\_1 \le 1$.
	- **<mark>Higher is better</mark>** : look at the formular of $F_1$ to see the reason. It needs both the precison and recall to be bigger. Other words, the False Negative is smaller (the wrong selected items are less) and the False Negative is smaller too (the wrong non-selected items are less).
	- **Hard to remember TP, TN, FP, FN?** read [this]({{site.url}}{{site.baseurl}}/understand-confusion-matrix-and-f1-score#remember-the-confusion-matrix){:target="_blank"}.

	~~~ python
  sklearn.metrics.f1_score
	~~~

- **Log Loss** : Logarithmic loss (also known as Log loss) measures the performance of a classifier where the predicted output is a probability value between 0 and 1.
	- We can calculate the log loss for each row using the *log loss equation*, which measures *how far each prediction is from the actual label*.

		$$
		LogLoss = -\frac{1}{n} \sum (y\times \log(\hat{y}) + (1-y)\times\log(1-\hat{y}) )
		$$
	
	- $1 \ge LogLoss \ge 0$.
	- **<mark>Smaller is better</mark>**.

	~~~ python
  sklearn.metrics.log_loss
	~~~

- **Accuracy score** : using in **scikit-learn**

  ~~~ python
  sklearn.metrics.accuracy_score(y_test, y_pred)
  # accuracy_score = jaccard_similarity_score (binary & muticlass classification)
  ~~~

{:.question}
## The idea of K-fold cross validation?

- We use CV to estimate how well a ML model would generalize to new data? It helps avoid [overfitting and underfitting]({{site.url}}{{site.baseurl}}/what-is-machine-learning#overfitting-underfitting).
- CV set and training set must <mark>use the same distribution</mark>! Why, check [this]({{site.url}}{{site.baseurl}}/what-is-machine-learning#diff-training-test-validation-tests){:target="_blank"}.
- We choose different groups of CV set/training set to find the predictions, after that, we choose the best one.

	{:.img-full-normal}
	![K-fold Cross Validation idea]({{img-url}}/k-fold.png)

~~~ python
from sklearn.model_selection import KFold
kf = KFold(n_splits=2)
kf.get_n_splits(X)
for train_index, test_index in kf.split(X):
	X_train, X_test = X[train_index], X[test_index]
	y_train, y_test = y[train_index], y[test_index]
~~~


{:.ref}
Source of figures used in this post: [k-fold](https://towardsdatascience.com/cross-validation-70289113a072){:target="_blank"}, [jaccard](https://thatware.co/jaccard-similarity/){:target="_blank"}, [f1-score](https://en.wikipedia.org/wiki/F1_score){:target="_blank"}.